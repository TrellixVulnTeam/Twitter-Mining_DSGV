{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from  textacy.vsm import Vectorizer\n",
    "\n",
    "from tqdm import *\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting all the tweets from the twitter api and then saving it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('./tweet.csv',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>username</th>\n",
       "      <th>retweets</th>\n",
       "      <th>text</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6/5/2019 3:26</td>\n",
       "      <td>mchellap</td>\n",
       "      <td>1</td>\n",
       "      <td>The western ghats policy that wasn't implement...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Keralafloods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6/4/2019 16:10</td>\n",
       "      <td>SRKKeralaFC</td>\n",
       "      <td>30</td>\n",
       "      <td>Schools are gonna open this week all over Kera...</td>\n",
       "      <td>@SRKCHENNAIFCpic</td>\n",
       "      <td>#KeralaFloods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6/4/2019 14:55</td>\n",
       "      <td>NewIndianXpress</td>\n",
       "      <td>2</td>\n",
       "      <td>Local self-government institutions and governm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Wayanad #KeralaFloods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6/3/2019 6:55</td>\n",
       "      <td>JustOutNews</td>\n",
       "      <td>0</td>\n",
       "      <td>Govt to construct four new dams in Kerala; aim...</td>\n",
       "      <td>@CPIMKerala @keralagovernment</td>\n",
       "      <td>#kerala #keralafloods #StateNews #CurrentUpdat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5/29/2019 16:22</td>\n",
       "      <td>Alonzo10541251</td>\n",
       "      <td>0</td>\n",
       "      <td>Thubten Chodron speaks against Dagri Rinpoche....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#AwardWapsiExposed #SonOfTadipar #instagood #l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date         username  retweets  \\\n",
       "0    6/5/2019 3:26         mchellap         1   \n",
       "1   6/4/2019 16:10      SRKKeralaFC        30   \n",
       "2   6/4/2019 14:55  NewIndianXpress         2   \n",
       "3    6/3/2019 6:55      JustOutNews         0   \n",
       "4  5/29/2019 16:22   Alonzo10541251         0   \n",
       "\n",
       "                                                text  \\\n",
       "0  The western ghats policy that wasn't implement...   \n",
       "1  Schools are gonna open this week all over Kera...   \n",
       "2  Local self-government institutions and governm...   \n",
       "3  Govt to construct four new dams in Kerala; aim...   \n",
       "4  Thubten Chodron speaks against Dagri Rinpoche....   \n",
       "\n",
       "                        mentions  \\\n",
       "0                            NaN   \n",
       "1               @SRKCHENNAIFCpic   \n",
       "2                            NaN   \n",
       "3  @CPIMKerala @keralagovernment   \n",
       "4                            NaN   \n",
       "\n",
       "                                            hashtags  \n",
       "0                                      #Keralafloods  \n",
       "1                                      #KeralaFloods  \n",
       "2                             #Wayanad #KeralaFloods  \n",
       "3  #kerala #keralafloods #StateNews #CurrentUpdat...  \n",
       "4  #AwardWapsiExposed #SonOfTadipar #instagood #l...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.dropna(axis=0, subset=['retweets'],inplace=True)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting text from the tweets dataframe\n",
    "\n",
    "Removing URLs, Removing @..., and the hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the corpus\n",
    "tweet_text = []\n",
    "tweets.text = tweets.text.apply(lambda x: re.sub(u'https:\\S+', u'', x))\n",
    "tweets.text = tweets.text.apply(lambda x: re.sub(u'http:\\S+', u'', x))\n",
    "tweets.text = tweets.text.apply(lambda x: re.sub(u'(\\s)@\\w+', u'', x))\n",
    "tweets.text = tweets.text.apply(lambda x: re.sub(u'#', u'', x))\n",
    "for text in tweets.text:\n",
    "    tweet_text.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Palakkad',\n",
       " 'Thala',\n",
       " 'Ajith',\n",
       " 'fans',\n",
       " 'Kodumbu',\n",
       " 'unit',\n",
       " 'provided',\n",
       " 'relief',\n",
       " 'materials',\n",
       " 'to',\n",
       " 'those',\n",
       " 'who',\n",
       " 'are',\n",
       " 'affected',\n",
       " 'by',\n",
       " 'KeralaFloods',\n",
       " 'pic.twitter.com/3sVnudB6Wt']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "\n",
    "nltk_tweets = []\n",
    "for text in tweets.text:\n",
    "    nltk_tweets.append(tknzr.tokenize(text))\n",
    "nltk_tweets[-68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using POS tagger to get the array of various part of speech in the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Palakkad', 'NNP'),\n",
      " ('Thala', 'NNP'),\n",
      " ('Ajith', 'NNP'),\n",
      " ('fans', 'NNS'),\n",
      " ('Kodumbu', 'NNP'),\n",
      " ('unit', 'NN'),\n",
      " ('provided', 'VBD'),\n",
      " ('relief', 'NN'),\n",
      " ('materials', 'NNS'),\n",
      " ('to', 'TO'),\n",
      " ('those', 'DT'),\n",
      " ('who', 'WP'),\n",
      " ('are', 'VBP'),\n",
      " ('affected', 'VBN'),\n",
      " ('by', 'IN'),\n",
      " ('KeralaFloods', 'NNP'),\n",
      " ('pic.twitter.com/3sVnudB6Wt', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "nltk_pos = []\n",
    "\n",
    "for text in nltk_tweets:\n",
    "    nltk_pos.append(pos_tag(text))\n",
    "pprint(nltk_pos[-68])\n",
    "#print(ne_chunk(nltk_pos[-68]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried Named entity recognition using NLTK but not accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "#cp = nltk.RegexpParser(pattern)\n",
    "#cs = cp.parse(nltk_pos[-68])\n",
    "#print(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iob_tagged= tree2conlltags(cs)\n",
    "#pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using Stanford Natural Processing!!\n",
    "First, we will set the config_java file for nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.internals.config_java(\"/usr/lib/jvm/java-11-openjdk-amd64/bin/java\")\n",
    "st = StanfordNERTagger('/home/pranav/Desktop/zine/Twitter-Mining/stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "           '/home/pranav/Desktop/zine/Twitter-Mining/stanford-ner-2018-10-16/stanford-ner.jar', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk_ents = []\n",
    "#for tweet in tqdm(nltk_tweets):\n",
    "#    entity_tagged_tweet = st.tag(tweet)\n",
    "#    nltk_ents.append([tag for tag in entity_tagged_tweet if tag[1] != 'O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Standford Named Entity Recognition library labels the text in the tweets, particularly into 3 classes (PERSON, ORGANIZATION, LOCATION).<br>\n",
    "As, numerals will also be significant in the tweets we will concatenate it to the entity text. Hence, from the text we will take care about the entities and numbers.<br>\n",
    "I will name these array content_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, entities that are labelled as PERSON tend to be related more to feelings of the person, hence I will remove them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_tweets = []\n",
    "for pos_tweet, tweet_entity in zip(nltk_pos, nltk_ents):\n",
    "    # starting by appending all of the entities\n",
    "    tweet_content = [word[0] for word in tweet_entity if word[1] != 'PERSON']\n",
    "    \n",
    "    # next by appending all of the numerals\n",
    "    for token in pos_tweet:\n",
    "         if token[1] == u'CD':\n",
    "            tweet_content.append(token[0])\n",
    "    content_tweets.append(tweet_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the tl-idf score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will take out tl-idf score for the tweet that will determine how much the word present in the tweet is importants.<br>\n",
    "So, I will take out the tl-idf score of all of the nlt_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectorizer = Vectorizer(weighting='tfidf')\n",
    "\n",
    "text_matrix = vectorizer.fit_transform(tweet_text)\n",
    "text_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
